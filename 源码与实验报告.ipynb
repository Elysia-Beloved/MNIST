{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "测试组合: Activation=relu, Init=default, Optimizer=sgd\n",
      "结果: Loss=0.0337, Accuracy=98.86%\n",
      "\n",
      "测试组合: Activation=relu, Init=default, Optimizer=adam\n",
      "结果: Loss=0.0313, Accuracy=98.99%\n",
      "\n",
      "测试组合: Activation=relu, Init=default, Optimizer=adadelta\n",
      "结果: Loss=0.0390, Accuracy=98.74%\n",
      "\n",
      "测试组合: Activation=relu, Init=xavier, Optimizer=sgd\n",
      "结果: Loss=0.0431, Accuracy=98.63%\n",
      "\n",
      "测试组合: Activation=relu, Init=xavier, Optimizer=adam\n",
      "结果: Loss=0.0357, Accuracy=98.85%\n",
      "\n",
      "测试组合: Activation=relu, Init=xavier, Optimizer=adadelta\n",
      "结果: Loss=0.0324, Accuracy=99.00%\n",
      "\n",
      "测试组合: Activation=relu, Init=he, Optimizer=sgd\n",
      "结果: Loss=0.0344, Accuracy=98.92%\n",
      "\n",
      "测试组合: Activation=relu, Init=he, Optimizer=adam\n",
      "结果: Loss=0.0555, Accuracy=98.37%\n",
      "\n",
      "测试组合: Activation=relu, Init=he, Optimizer=adadelta\n",
      "结果: Loss=0.0364, Accuracy=98.78%\n",
      "\n",
      "测试组合: Activation=tanh, Init=default, Optimizer=sgd\n",
      "结果: Loss=0.0479, Accuracy=98.61%\n",
      "\n",
      "测试组合: Activation=tanh, Init=default, Optimizer=adam\n",
      "结果: Loss=0.0445, Accuracy=98.54%\n",
      "\n",
      "测试组合: Activation=tanh, Init=default, Optimizer=adadelta\n",
      "结果: Loss=0.0443, Accuracy=98.62%\n",
      "\n",
      "测试组合: Activation=tanh, Init=xavier, Optimizer=sgd\n",
      "结果: Loss=0.0482, Accuracy=98.52%\n",
      "\n",
      "测试组合: Activation=tanh, Init=xavier, Optimizer=adam\n",
      "结果: Loss=0.0569, Accuracy=98.31%\n",
      "\n",
      "测试组合: Activation=tanh, Init=xavier, Optimizer=adadelta\n",
      "结果: Loss=0.0432, Accuracy=98.56%\n",
      "\n",
      "测试组合: Activation=tanh, Init=he, Optimizer=sgd\n",
      "结果: Loss=0.0490, Accuracy=98.42%\n",
      "\n",
      "测试组合: Activation=tanh, Init=he, Optimizer=adam\n",
      "结果: Loss=0.0490, Accuracy=98.47%\n",
      "\n",
      "测试组合: Activation=tanh, Init=he, Optimizer=adadelta\n",
      "结果: Loss=0.0477, Accuracy=98.39%\n",
      "\n",
      "测试组合: Activation=sigmoid, Init=default, Optimizer=sgd\n",
      "结果: Loss=0.2419, Accuracy=92.90%\n",
      "\n",
      "测试组合: Activation=sigmoid, Init=default, Optimizer=adam\n",
      "结果: Loss=0.0683, Accuracy=98.06%\n",
      "\n",
      "测试组合: Activation=sigmoid, Init=default, Optimizer=adadelta\n",
      "结果: Loss=0.1252, Accuracy=96.10%\n",
      "\n",
      "测试组合: Activation=sigmoid, Init=xavier, Optimizer=sgd\n",
      "结果: Loss=0.2463, Accuracy=92.63%\n",
      "\n",
      "测试组合: Activation=sigmoid, Init=xavier, Optimizer=adam\n",
      "结果: Loss=2.3029, Accuracy=10.09%\n",
      "\n",
      "测试组合: Activation=sigmoid, Init=xavier, Optimizer=adadelta\n",
      "结果: Loss=0.1536, Accuracy=95.07%\n",
      "\n",
      "测试组合: Activation=sigmoid, Init=he, Optimizer=sgd\n",
      "结果: Loss=0.1971, Accuracy=94.45%\n",
      "\n",
      "测试组合: Activation=sigmoid, Init=he, Optimizer=adam\n",
      "结果: Loss=0.0598, Accuracy=98.34%\n",
      "\n",
      "测试组合: Activation=sigmoid, Init=he, Optimizer=adadelta\n",
      "结果: Loss=0.0552, Accuracy=98.29%\n",
      "\n",
      "**性能表格**\n",
      "   Activation Initialization Optimizer  Test Loss  Accuracy (%)\n",
      "0        relu        default       sgd   0.033739         98.86\n",
      "1        relu        default      adam   0.031328         98.99\n",
      "2        relu        default  adadelta   0.038972         98.74\n",
      "3        relu         xavier       sgd   0.043064         98.63\n",
      "4        relu         xavier      adam   0.035740         98.85\n",
      "5        relu         xavier  adadelta   0.032425         99.00\n",
      "6        relu             he       sgd   0.034412         98.92\n",
      "7        relu             he      adam   0.055457         98.37\n",
      "8        relu             he  adadelta   0.036410         98.78\n",
      "9        tanh        default       sgd   0.047940         98.61\n",
      "10       tanh        default      adam   0.044486         98.54\n",
      "11       tanh        default  adadelta   0.044337         98.62\n",
      "12       tanh         xavier       sgd   0.048222         98.52\n",
      "13       tanh         xavier      adam   0.056893         98.31\n",
      "14       tanh         xavier  adadelta   0.043230         98.56\n",
      "15       tanh             he       sgd   0.048998         98.42\n",
      "16       tanh             he      adam   0.049032         98.47\n",
      "17       tanh             he  adadelta   0.047721         98.39\n",
      "18    sigmoid        default       sgd   0.241883         92.90\n",
      "19    sigmoid        default      adam   0.068304         98.06\n",
      "20    sigmoid        default  adadelta   0.125214         96.10\n",
      "21    sigmoid         xavier       sgd   0.246325         92.63\n",
      "22    sigmoid         xavier      adam   2.302886         10.09\n",
      "23    sigmoid         xavier  adadelta   0.153630         95.07\n",
      "24    sigmoid             he       sgd   0.197114         94.45\n",
      "25    sigmoid             he      adam   0.059825         98.34\n",
      "26    sigmoid             he  adadelta   0.055219         98.29\n",
      "结果已保存到 `mnist_performance_results.csv`\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#权重初始化\n",
    "def initialize_weights(model, init_type):\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            if init_type == 'xavier':\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "            elif init_type == 'he':\n",
    "                nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "            elif init_type == 'default':\n",
    "                pass\n",
    "            if layer.bias is not None:\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "\n",
    "#激活函数选择\n",
    "def get_activation(name):\n",
    "    if name == 'relu':\n",
    "        return F.relu\n",
    "    elif name == 'tanh':\n",
    "        return torch.tanh\n",
    "    elif name == 'sigmoid':\n",
    "        return torch.sigmoid\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "\n",
    "#定义网络\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, activation):\n",
    "        super(Net, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "#训练\n",
    "def train(model, device, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "#测试\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy\n",
    "\n",
    "\n",
    "#主程序\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    #数据加载\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=True, download=True, transform=transform),\n",
    "        batch_size=64, shuffle=True) # 批量大小为64，随机打乱\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=False, transform=transform),\n",
    "        batch_size=1000, shuffle=False)# 批量大小为1000，按顺序加载\n",
    "\n",
    "    #参数组合\n",
    "    activations = ['relu', 'tanh', 'sigmoid']\n",
    "    inits = ['default', 'xavier', 'he']\n",
    "    optimizers = ['sgd', 'adam', 'adadelta']\n",
    "    results = []\n",
    "\n",
    "    #遍历所有组合\n",
    "    for act, init, opt in product(activations, inits, optimizers):\n",
    "        print(f\"\\n测试组合: Activation={act}, Init={init}, Optimizer={opt}\")\n",
    "        model = Net(get_activation(act)).to(device)\n",
    "        initialize_weights(model, init)\n",
    "\n",
    "        #优化器选择\n",
    "        optimizer = {\n",
    "            'sgd': optim.SGD(model.parameters(), lr=0.01, momentum=0.9),\n",
    "            'adam': optim.Adam(model.parameters(), lr=0.001),\n",
    "            'adadelta': optim.Adadelta(model.parameters(), lr=1.0)\n",
    "        }[opt]\n",
    "        scheduler = StepLR(optimizer, step_size=1, gamma=0.7) # 学习率调度器\n",
    "\n",
    "        #训练&测试\n",
    "        for epoch in range(1, 3):  #简化为2个epoch\n",
    "            train(model, device, train_loader, optimizer)\n",
    "            scheduler.step()\n",
    "\n",
    "        test_loss, accuracy = test(model, device, test_loader)\n",
    "        print(f\"结果: Loss={test_loss:.4f}, Accuracy={accuracy:.2f}%\")\n",
    "\n",
    "        results.append({\n",
    "            'Activation': act,\n",
    "            'Initialization': init,\n",
    "            'Optimizer': opt,\n",
    "            'Test Loss': test_loss,\n",
    "            'Accuracy (%)': accuracy\n",
    "        })\n",
    "\n",
    "    #保存结果\n",
    "    df = pd.DataFrame(results)\n",
    "    print(\"\\n**性能表格**\")\n",
    "    print(df)\n",
    "    df.to_csv('mnist_performance_results.csv', index=False)\n",
    "    print(\"结果已保存到 `mnist_performance_results.csv`\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果分析\n",
    "1. 激活函数对结果的影响\n",
    "ReLU :\n",
    "\n",
    "表现非常好，在所有优化器和初始化方法的组合中，ReLU的准确率普遍较高 (最高达到 99%)，表明其在梯度传播和网络收敛方面有优势。\n",
    "最优组合是 ReLU + Xavier + Adadelta，测试损失为0.0324，准确率达到了99%。\n",
    "Tanh :\n",
    "\n",
    "其性能略逊于ReLU，准确率整体偏低（最高为98.62%）。\n",
    "Tanh更适合在较浅层网络或需要对数据进行标准化的场景，但相比ReLU，在深层网络中容易出现梯度消失问题。\n",
    "测试结果显示Tanh对初始化方法和优化器的选择较为敏感，表现最优的组合是 Tanh + Default + Adadelta，准确率为98.62%。\n",
    "Sigmoid :\n",
    "\n",
    "表现最差，尤其是在 Sigmoid + Xavier + Adam 组合下，准确率降到10.09%（几乎是随机分类）。这是因为Sigmoid函数在深层网络中容易出现梯度消失问题。\n",
    "但在某些特定组合下（如 Sigmoid + He + Adam），测试损失低至0.0598，准确率达到了98.34%，显示在一定条件下Sigmoid仍有潜力。\n",
    "2. 权重初始化对结果的影响\n",
    "Default :\n",
    "\n",
    "在ReLU和Tanh的组合中，Default初始化表现相对较好。\n",
    "在Sigmoid的情况下，Default初始化的表现明显优于Xavier，测试损失更小，准确率也更高。\n",
    "Xavier :\n",
    "\n",
    "Xavier初始化在ReLU和Tanh的情况下表现较稳定，尤其是与Adadelta优化器结合时，达到最高准确率（99%）。\n",
    "但在Sigmoid的情况下，Xavier初始化表现非常糟糕（如Sigmoid + Xavier + Adam的测试损失为2.302，准确率仅为10.09%），因为Sigmoid本身的梯度衰减与Xavier的初始化策略不匹配。\n",
    "He :\n",
    "\n",
    "He初始化专为ReLU设计，因此在ReLU的组合下表现非常优异，尤其是ReLU + He + SGD（准确率98.92%）。\n",
    "然而在Sigmoid和Tanh的情况下，He初始化的表现不如其他方法。\n",
    "3. 优化器对结果的影响\n",
    "SGD :\n",
    "\n",
    "表现稳定，但收敛速度较慢，尤其是在ReLU和Tanh的情况下，测试损失普遍较大（如Tanh + He + SGD的损失为0.0489）。\n",
    "对于Sigmoid函数，SGD表现相对较差。\n",
    "Adam:\n",
    "\n",
    "通常表现优于SGD。\n",
    "在ReLU的情况下，Adam优化器能达到较高的准确率（如ReLU + Default + Adam的准确率98.99%）。\n",
    "但在Sigmoid + Xavier的情况下，Adam表现非常糟糕（准确率仅为10.09%），这可能是因为初始化方法和激活函数的选择导致优化陷入了局部最优解。\n",
    "Adadelta:\n",
    "\n",
    "表现非常优秀，尤其是在ReLU + Xavier的情况下达到了最高准确率99%。\n",
    "Adadelta在调整学习率方面表现较好，尤其是对Sigmoid和Tanh等容易出现梯度消失问题的激活函数，具有较好的效果。\n",
    "4. 结论：\n",
    "\n",
    "ReLU 激活函数的表现最优：\n",
    "它在大多数情况下能提供更好的性能，特别是与Xavier或He初始化方法结合时效果显著。\n",
    "对于深度学习任务，建议优先选择ReLU。\n",
    "\n",
    "初始化方法的选择需与激活函数匹配：\n",
    "ReLU适合He和Xavier初始化，Sigmoid适合Default和He初始化，Tanh对初始化方法的选择较为宽容，但总体表现不及ReLU。\n",
    "\n",
    "优化器选择因模型而异：\n",
    "Adam优化器表现普遍较好，但对初始化方法敏感。\n",
    "Adadelta在某些情况下（如ReLU + Xavier）可以实现更优的性能，尤其是对梯度消失问题具有良好效果。\n",
    "\n",
    "Sigmoid激活函数需谨慎使用：\n",
    "在深度网络中，Sigmoid激活函数表现差强人意，容易出现梯度消失问题，导致训练效果不佳。可以尝试改用ReLU或Tanh。\n",
    "\n",
    "总体而言，最佳组合为：ReLU + Xavier + Adadelta，达到了最低测试损失（0.0324）和最高准确率（99%）。\n",
    "当然这并不是普适结论，而是特定环境里的特定结果，更改参数，环境，数据集，结果可能大有不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1...\n",
      "Epoch 2...\n",
      "Epoch 3...\n",
      "Epoch 4...\n",
      "Epoch 5...\n",
      "Epoch 6...\n",
      "Epoch 7...\n",
      "Epoch 8...\n",
      "Epoch 9...\n",
      "Epoch 10...\n",
      "Epoch 11...\n",
      "Epoch 12...\n",
      "Epoch 13...\n",
      "Epoch 14...\n",
      "Epoch 15...\n",
      "Sigmoid + Xavier + Adam 组合 - 结果: Loss=0.0418, Accuracy=98.67%\n",
      "  Activation Initialization Optimizer  Test Loss  Accuracy (%)\n",
      "0    sigmoid         xavier      adam   0.041775         98.67\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#权重初始化\n",
    "def initialize_weights(model, init_type):\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "            if init_type == 'xavier':\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "            elif init_type == 'he':\n",
    "                nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "            elif init_type == 'default':\n",
    "                pass\n",
    "            if layer.bias is not None:\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "\n",
    "#激活函数选择\n",
    "def get_activation(name):\n",
    "    if name == 'relu':\n",
    "        return F.relu\n",
    "    elif name == 'tanh':\n",
    "        return torch.tanh\n",
    "    elif name == 'sigmoid':\n",
    "        return torch.sigmoid\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported activation function\")\n",
    "\n",
    "\n",
    "#定义网络\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, activation):\n",
    "        super(Net, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.activation(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "#训练\n",
    "def train(model, device, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "#测试\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy\n",
    "\n",
    "\n",
    "#主程序\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    #数据加载\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=True, download=True, transform=transform),\n",
    "        batch_size=64, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=False, transform=transform),\n",
    "        batch_size=1000, shuffle=False)\n",
    "\n",
    "    #激活函数，初始化方式和优化器\n",
    "    activation = 'sigmoid'\n",
    "    init_type = 'xavier'\n",
    "    optimizer_type = 'adam'\n",
    "\n",
    "    #初始化模型\n",
    "    model = Net(get_activation(activation)).to(device)\n",
    "    initialize_weights(model, init_type)\n",
    "\n",
    "    #优化器选择\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "\n",
    "    #训练 15 轮\n",
    "    for epoch in range(1, 16):  \n",
    "        print(f\"Epoch {epoch}...\")\n",
    "        train(model, device, train_loader, optimizer)\n",
    "        scheduler.step()\n",
    "\n",
    "    #测试\n",
    "    test_loss, accuracy = test(model, device, test_loader)\n",
    "    print(f\"Sigmoid + Xavier + Adam 组合 - 结果: Loss={test_loss:.4f}, Accuracy={accuracy:.2f}%\")\n",
    "\n",
    "    #保存结果\n",
    "    results = [{\n",
    "        'Activation': activation,\n",
    "        'Initialization': init_type,\n",
    "        'Optimizer': optimizer_type,\n",
    "        'Test Loss': test_loss,\n",
    "        'Accuracy (%)': accuracy\n",
    "    }]\n",
    "    df = pd.DataFrame(results)\n",
    "    print(df)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意到前面Sigmoid + Xavier + Adam 组合表现非常糟糕（准确率仅为10.09%），但又不可能是sigmoid因为深度网络导致梯度消失（因为为了节省时间我只设了2个epoch），因此怀疑是性能差的主要原因可能是训练时间不足，模型还没有充分学习和调整。而结果也正如所料，提高了训练轮数后，该组合表现也还不错。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
